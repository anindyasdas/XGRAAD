# Unmasking Backdoors: An Explainable Defense via Gradientâ€“Attention Anomaly Scoring for Pre-trained Language Models

This is the official repository for the paper:

> **Unmasking Backdoors: An Explainable Defense via Gradientâ€“Attention Anomaly Scoring for Pre-trained Language Models**

**X-GRAAD** introduces an inference-time defense that detects and mitigates backdoor attacks in pre-trained NLP models by computing anomaly scores from token-level attention and gradient signals. These signals highlight trigger tokens that dominate the modelâ€™s internal behavior.

The method reduces attack success rates across diverse backdoor scenarios while also providing interpretable insights into trigger localization and defense effectiveness.

---

## ğŸ“¦ Dependencies

X-GRAAD is implemented and tested under the following environment:

```
Python 3.9
CUDA: 13.0
NVIDIA A100-SXM4-40GB
```

Install required packages:

```bash
pip install -r requirements.txt
```

---

## ğŸ›  Preparation

### Model Directory Structure

Trained backdoored models should be placed in the following directory structure:

```
trained_models1/
â”œâ”€â”€ BADNLI/
â”‚   â”œâ”€â”€ SST/
â”‚   â”œâ”€â”€ AGNews/
â”‚   â”œâ”€â”€ IMDb/
â”œâ”€â”€ RIPPLES/
â”‚   â”œâ”€â”€ SST/
```

> **Note:** Trained models are not included in this repository.  
> Please download or generate the poisoned models and place them in the above directory structure.

---

## ğŸ§ª Backdoor Attacks

All backdoor attacks are implemented using the open-source toolkit  
[OpenBackdoor](https://github.com/thunlp/OpenBackdoor).

### Supported Attacks

- **BADNLI (BadNets)**
- **RIPPLES**
- **LWS**

---

## ğŸ›¡ Defense: X-GRAAD Framework

### Detection & Defense Module

This module identifies poisoned inputs by computing anomaly scores derived from gradient and attention signals, followed by threshold-based filtering.

### Example Usage

```bash
python x_graad.py \
    --model_name bert \
    --ds_name sst \
    --attack_name BADNLI \
    --threshold_percentile 95 \
    --trigger_id 0
```

---

## ğŸ”§ Supported Arguments

### Datasets (`--ds_name`)
- `imdb`
- `sst`
- `ag_news`

### Models (`--model_name`)
- `bert`
- `distilbert`
- `albert`
- `roberta`
- `deberta`

### Attacks (`--attack_name`)
- `BADNLI`
- `RIPPLES`
- `LWS`

---

## ğŸ¯ Trigger Configurations (`--trigger_id`)

```
0: ["cf"]
1: ["cf", "tq", "mn", "bb", "mb"]   # multiple triggers
2: ["james bond"]
3: ["velvet shadow"]
4: ["the silver quill"]
5: ["a crimson echo"]
```

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ trained_models1/         # Directory for trained (poisoned) model weights (not included)
â”œâ”€â”€ util.py                  # Utility functions
â”œâ”€â”€ x_graad.py               # Main detection and defense pipeline
â”œâ”€â”€ config.py                # Configuration definitions
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md                # Project documentation
```

---

## ğŸ“Œ Notes

- This repository focuses on inference-time backdoor detection and mitigation.
- The trained backdoored models are not distributed in this repository.
- For attack generation, please refer to OpenBackdoor.

---

## ğŸ“œ License

This project is released for research purposes only.

---

## ğŸ“– Citation

If you find this work useful, please cite:
```
@article{das2025unmasking,
  title={Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models},
  author={Das, Anindya Sundar and Chen, Kangjie and Bhuyan, Monowar},
  journal={arXiv preprint arXiv:2510.04347},
  year={2025}
}
```
